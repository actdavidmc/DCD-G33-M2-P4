{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Practica 4 - Base Unificada (Listings)\n",
        "\n",
        "Objetivo: construir una base unificada con **listings.csv** y aplicar tecnicas vistas en los notebooks `source/##_*.ipynb`.\n",
        "\n",
        "- Target principal: **price** (regresion)\n",
        "- Dataset: **local** `listings.csv`\n",
        "- Incluye: EDA profundo + diccionario de datos, limpieza, ingenieria de variables, seleccion de variables,\n",
        "  modelos lineales/regularizados, KNN, SVM/SVR, Kernel Ridge, SGD, arboles, ensambles, redes neuronales,\n",
        "  y una seccion auxiliar de clasificacion por segmentos de precio para aplicar LDA/Naive Bayes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# === 1) Imports y configuracion ===\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "# Modelos regresion\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, SGDRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor,\n",
        "    VotingRegressor\n",
        ")\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Modelos clasificacion (auxiliar)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Metricas\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import accuracy_score, f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR = Path(\"data\")\n",
        "\n",
        "if not Path(DATA_PATH).exists():\n",
        "    raise FileNotFoundError(f\"No existe {DATA_PATH} en el directorio actual\")\n",
        "\n",
        "raw = pd.read_csv(DATA_DIR / \"listings.csv.gz\", compression=\"gzip\", low_memory=False)\n",
        "print(\"Shape:\", raw.shape)\n",
        "raw.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) EDA profundo y diccionario de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Vista rapida\n",
        "raw.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Resumen de nulos\n",
        "missing = (raw.isna().mean() * 100).sort_values(ascending=False)\n",
        "missing.head(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Diccionario de datos\n",
        "\n",
        "def build_data_dictionary(df):\n",
        "    rows = []\n",
        "    n = len(df)\n",
        "    for col in df.columns:\n",
        "        s = df[col]\n",
        "        dtype = str(s.dtype)\n",
        "        n_missing = s.isna().sum()\n",
        "        pct_missing = (n_missing / n) * 100\n",
        "        n_unique = s.nunique(dropna=True)\n",
        "        pct_unique = (n_unique / n) * 100\n",
        "        sample = s.dropna().astype(str).head(3).tolist()\n",
        "        mean_len = None\n",
        "        if s.dtype == 'object':\n",
        "            mean_len = s.dropna().astype(str).str.len().mean()\n",
        "        rows.append({\n",
        "            'column': col,\n",
        "            'dtype': dtype,\n",
        "            'n_missing': n_missing,\n",
        "            'pct_missing': round(pct_missing, 2),\n",
        "            'n_unique': n_unique,\n",
        "            'pct_unique': round(pct_unique, 2),\n",
        "            'mean_len': round(mean_len, 2) if mean_len else None,\n",
        "            'sample': sample\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "data_dict = build_data_dictionary(raw)\n",
        "data_dict.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Heuristicas para candidatos a eliminar\n",
        "\n",
        "def detect_drop_candidates(df, missing_threshold=60, high_card_threshold=200, long_text_len=80):\n",
        "    rows = []\n",
        "    n = len(df)\n",
        "    for col in df.columns:\n",
        "        s = df[col]\n",
        "        name = col.lower()\n",
        "        n_unique = s.nunique(dropna=True)\n",
        "        pct_unique = (n_unique / n) * 100 if n else 0\n",
        "        pct_missing = s.isna().mean() * 100\n",
        "        reasons = []\n",
        "\n",
        "        # Constantes\n",
        "        if n_unique <= 1:\n",
        "            reasons.append('constant')\n",
        "\n",
        "        # Muchos nulos\n",
        "        if pct_missing > missing_threshold:\n",
        "            reasons.append(f'missing>{missing_threshold}%')\n",
        "\n",
        "        # IDs / URL / metadata\n",
        "        if 'url' in name or s.astype(str).str.contains('http', case=False, na=False).mean() > 0.2:\n",
        "            reasons.append('url_like')\n",
        "        if 'scrape' in name:\n",
        "            reasons.append('scrape_meta')\n",
        "        if name == 'id' or name.endswith('id') or 'id_' in name:\n",
        "            if pct_unique > 90:\n",
        "                reasons.append('id_like')\n",
        "\n",
        "        # Texto largo / alta cardinalidad\n",
        "        if s.dtype == 'object':\n",
        "            mean_len = s.dropna().astype(str).str.len().mean()\n",
        "            if mean_len and mean_len > long_text_len:\n",
        "                reasons.append('long_text')\n",
        "            if n_unique > high_card_threshold:\n",
        "                reasons.append('high_cardinality')\n",
        "\n",
        "        if reasons:\n",
        "            rows.append({\n",
        "                'column': col,\n",
        "                'reasons': ', '.join(reasons),\n",
        "                'pct_missing': round(pct_missing, 2),\n",
        "                'pct_unique': round(pct_unique, 2),\n",
        "                'n_unique': n_unique\n",
        "            })\n",
        "\n",
        "    report = pd.DataFrame(rows).sort_values(by=['pct_missing', 'n_unique'], ascending=False)\n",
        "    candidates = report['column'].tolist()\n",
        "    return report, candidates\n",
        "\n",
        "\n",
        "drop_report, drop_candidates = detect_drop_candidates(raw)\n",
        "\n",
        "# Columnas que NO queremos eliminar aunque salgan como candidatas\n",
        "protect_cols = ['price', 'amenities', 'neighbourhood_cleansed', 'property_type', 'room_type']\n",
        "drop_candidates = [c for c in drop_candidates if c not in protect_cols]\n",
        "\n",
        "drop_report.head(20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Candidatos a eliminar (resumen)\n",
        "print(\"Total candidatos:\", len(drop_candidates))\n",
        "print(drop_candidates[:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Columnas relacionadas con price (posible leakage si se usan como features)\n",
        "price_cols = [c for c in raw.columns if 'price' in c.lower()]\n",
        "price_cols\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlaciones con el target (Spearman) - numericas\n",
        "eda_df = raw.copy()\n",
        "\n",
        "# price_clean temporal para EDA\n",
        "if 'price' in eda_df.columns:\n",
        "    eda_df['price_clean'] = (\n",
        "        eda_df['price'].astype(str)\n",
        "        .str.replace(r\"[,\\$]\", \"\", regex=True)\n",
        "        .str.strip()\n",
        "    )\n",
        "    eda_df['price_clean'] = pd.to_numeric(eda_df['price_clean'], errors='coerce')\n",
        "\n",
        "num_cols = eda_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "num_cols = [c for c in num_cols if c != 'price_clean']\n",
        "\n",
        "corr = eda_df[num_cols].corrwith(eda_df['price_clean'], method='spearman').sort_values(ascending=False)\n",
        "print(\"Top correlaciones positivas:\")\n",
        "print(corr.head(10))\n",
        "print(\"\")\n",
        "print(\"Top correlaciones negativas:\")\n",
        "print(corr.tail(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Visualizacion de top correlaciones (absolutas)\n",
        "if len(corr) > 0:\n",
        "    top_corr = corr.abs().sort_values(ascending=False).head(15)\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    top_corr.sort_values().plot(kind='barh')\n",
        "    plt.title('Top correlaciones (Spearman) con price_clean')\n",
        "    plt.xlabel('abs(corr)')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Limpieza y feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Copia de trabajo\n",
        "_df = raw.copy()\n",
        "\n",
        "# Limpieza de price -> numerico\n",
        "if 'price' in _df.columns:\n",
        "    _df['price_clean'] = (\n",
        "        _df['price'].astype(str)\n",
        "        .str.replace(r\"[,\\$]\", \"\", regex=True)\n",
        "        .str.strip()\n",
        "    )\n",
        "    _df['price_clean'] = pd.to_numeric(_df['price_clean'], errors='coerce')\n",
        "else:\n",
        "    raise ValueError(\"La columna 'price' no existe en listings.csv\")\n",
        "\n",
        "# Bathrooms desde bathrooms_text (si existe)\n",
        "if 'bathrooms_text' in _df.columns:\n",
        "    _df['bathrooms_count'] = (\n",
        "        _df['bathrooms_text'].astype(str)\n",
        "        .str.extract(r\"(\\d+\\.?\\d*)\")[0]\n",
        "        .astype(float)\n",
        "    )\n",
        "\n",
        "# Amenities -> lista y conteo\n",
        "if 'amenities' in _df.columns:\n",
        "    def _clean_amenities(text):\n",
        "        if pd.isna(text):\n",
        "            return []\n",
        "        text = text.replace('[', '').replace(']', '').replace('\"', '')\n",
        "        return [x.strip() for x in text.split(',') if x.strip()]\n",
        "\n",
        "    _df['amenities_list'] = _df['amenities'].apply(_clean_amenities)\n",
        "    _df['amenities_count'] = _df['amenities_list'].apply(len)\n",
        "\n",
        "    # Algunas amenities binarias\n",
        "    for amenity in [\n",
        "        'Wifi', 'Air conditioning', 'Kitchen', 'Pool', 'Parking', 'Washer', 'Dryer'\n",
        "    ]:\n",
        "        col = f\"has_{amenity.lower().replace(' ', '_')}\"\n",
        "        _df[col] = _df['amenities_list'].apply(\n",
        "            lambda x: 1 if any(amenity.lower() in item.lower() for item in x) else 0\n",
        "        )\n",
        "\n",
        "# Variables binarias tipicas\n",
        "for col in ['host_is_superhost', 'instant_bookable']:\n",
        "    if col in _df.columns:\n",
        "        _df[col] = _df[col].astype(str).str.lower().map({'t': 1, 'f': 0})\n",
        "\n",
        "# Features de capacidad\n",
        "for c in ['accommodates', 'bedrooms', 'beds']:\n",
        "    if c not in _df.columns:\n",
        "        _df[c] = np.nan\n",
        "\n",
        "_df['total_capacity'] = _df['accommodates'].fillna(0) + _df['bedrooms'].fillna(0) + _df['beds'].fillna(0)\n",
        "_df['bed_per_person'] = _df['beds'] / (_df['accommodates'].replace(0, np.nan))\n",
        "_df['bedroom_per_person'] = _df['bedrooms'] / (_df['accommodates'].replace(0, np.nan))\n",
        "\n",
        "# Reviews y calidad\n",
        "for c in ['review_scores_rating', 'number_of_reviews', 'reviews_per_month']:\n",
        "    if c not in _df.columns:\n",
        "        _df[c] = np.nan\n",
        "\n",
        "_df['reviews_per_month'] = pd.to_numeric(_df['reviews_per_month'], errors='coerce')\n",
        "\n",
        "# Ubicacion: distancia al centro (CDMX aproximado)\n",
        "if 'latitude' in _df.columns and 'longitude' in _df.columns:\n",
        "    center_lat, center_lon = 19.4326, -99.1332\n",
        "    _df['distance_from_center'] = np.sqrt(\n",
        "        (_df['latitude'] - center_lat)**2 + (_df['longitude'] - center_lon)**2\n",
        "    )\n",
        "    _df['is_central_location'] = (_df['distance_from_center'] < 0.05).astype(int)\n",
        "\n",
        "# Booking / disponibilidad\n",
        "for c in ['minimum_nights', 'maximum_nights', 'availability_365']:\n",
        "    if c not in _df.columns:\n",
        "        _df[c] = np.nan\n",
        "\n",
        "_df['booking_flexibility'] = _df['maximum_nights'] - _df['minimum_nights']\n",
        "_df['availability_rate'] = _df['availability_365'] / 365\n",
        "_df['scarcity_score'] = 1 - _df['availability_rate']\n",
        "\n",
        "# Log transforms\n",
        "for c in ['price_clean', 'number_of_reviews', 'reviews_per_month']:\n",
        "    if c in _df.columns:\n",
        "        _df[f'log_{c}'] = np.log1p(_df[c])\n",
        "\n",
        "\n",
        "\n",
        "# Amenidades (todas) -> columnas binarias\n",
        "if 'amenities_list' in _df.columns:\n",
        "    # Lista completa de amenidades\n",
        "    flat = [a for row in _df['amenities_list'] for a in row]\n",
        "    amenity_counts = pd.Series(flat).value_counts()\n",
        "    all_amenities = amenity_counts.index.tolist()\n",
        "\n",
        "    def _sanitize_amenity(name: str) -> str:\n",
        "        name = name.strip().lower()\n",
        "        name = re.sub(r'[^0-9a-zA-Z]+', '_', name)\n",
        "        return 'amenity_' + name.strip('_')\n",
        "\n",
        "    # Evitar colisiones de nombres\n",
        "    col_map = {}\n",
        "    used = set()\n",
        "    for a in all_amenities:\n",
        "        base = _sanitize_amenity(a)\n",
        "        col = base\n",
        "        k = 2\n",
        "        while col in used:\n",
        "            col = f\"{base}_{k}\"\n",
        "            k += 1\n",
        "        col_map[a] = col\n",
        "        used.add(col)\n",
        "\n",
        "    # Crear matriz binaria (sparse) para todas las amenidades\n",
        "    from sklearn.preprocessing import MultiLabelBinarizer\n",
        "    mlb = MultiLabelBinarizer(classes=all_amenities, sparse_output=True)\n",
        "    amenity_mat = mlb.fit_transform(_df['amenities_list'])\n",
        "\n",
        "    amenity_cols = [col_map[a] for a in mlb.classes_]\n",
        "    amenity_df = pd.DataFrame.sparse.from_spmatrix(amenity_mat, index=_df.index, columns=amenity_cols)\n",
        "    amenity_df = amenity_df.astype('int8')\n",
        "\n",
        "    _df = pd.concat([_df, amenity_df], axis=1)\n",
        "\n",
        "    print('Total amenidades creadas:', len(amenity_cols))\n",
        "\n",
        "print(\"Shape despues de FE:\", _df.shape)\n",
        "_df[['price_clean']].describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlacion de amenidades con price_clean\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "amenity_cols = [c for c in _df.columns if c.startswith('amenity_') or c.startswith('has_')]\n",
        "if amenity_cols:\n",
        "    corr_amen = _df[amenity_cols].corrwith(_df['price_clean'], method='spearman').sort_values(ascending=False)\n",
        "    print('Top correlaciones positivas (amenities):')\n",
        "    print(corr_amen.head(10))\n",
        "    print('')\n",
        "    print('Top correlaciones negativas (amenities):')\n",
        "    print(corr_amen.tail(10))\n",
        "\n",
        "    # Grafica top 15\n",
        "    top = corr_amen.abs().sort_values(ascending=False).head(15)\n",
        "    if len(top) > 0:\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        top.sort_values().plot(kind='barh')\n",
        "        plt.title('Top correlaciones (amenities) con price_clean')\n",
        "        plt.xlabel('abs(corr)')\n",
        "        plt.show()\n",
        "else:\n",
        "    print('No se generaron columnas de amenidades.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Manejo de nulos y outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Eliminar filas sin target\n",
        "_df = _df.dropna(subset=['price_clean']).copy()\n",
        "\n",
        "# Clip outliers en target\n",
        "p01, p99 = _df['price_clean'].quantile([0.01, 0.99])\n",
        "_df['price_clean'] = _df['price_clean'].clip(p01, p99)\n",
        "\n",
        "# Clip outliers en variables numericas (opcional y suave)\n",
        "num_cols = _df.select_dtypes(include=[np.number]).columns\n",
        "for col in num_cols:\n",
        "    if col == 'price_clean':\n",
        "        continue\n",
        "    q01, q99 = _df[col].quantile([0.01, 0.99])\n",
        "    if np.isfinite(q01) and np.isfinite(q99):\n",
        "        _df[col] = _df[col].clip(q01, q99)\n",
        "\n",
        "print(\"Shape final modelado:\", _df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Preparacion de datos para modelado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir target y features\n",
        "TARGET = 'price_clean'\n",
        "\n",
        "# Columnas base a excluir\n",
        "exclude_cols_base = [\n",
        "    'price', 'price_clean', 'amenities', 'amenities_list',\n",
        "    'log_price_clean', 'id', 'listing_url', 'scrape_id', 'last_scraped',\n",
        "    'name', 'description', 'picture_url', 'host_name'\n",
        "]\n",
        "\n",
        "# Excluir candidatos del EDA\n",
        "try:\n",
        "    exclude_cols_base = list(set(exclude_cols_base + drop_candidates))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Excluir texto largo o alta cardinalidad para controlar el one-hot\n",
        "obj_cols = _df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "def is_list_like_col(s, sample=200):\n",
        "    sample_s = s.dropna().head(sample)\n",
        "    return sample_s.apply(lambda x: isinstance(x, (list, dict, set))).any()\n",
        "\n",
        "list_like_cols = [c for c in obj_cols if is_list_like_col(_df[c])]\n",
        "obj_cols_safe = [c for c in obj_cols if c not in list_like_cols]\n",
        "\n",
        "high_card_cols = [c for c in obj_cols_safe if _df[c].nunique(dropna=True) > 200]\n",
        "\n",
        "long_text_cols = []\n",
        "for c in obj_cols_safe:\n",
        "    mean_len = _df[c].dropna().astype(str).str.len().mean()\n",
        "    if mean_len and mean_len > 80:\n",
        "        long_text_cols.append(c)\n",
        "\n",
        "exclude_cols = sorted(set(exclude_cols_base + high_card_cols + long_text_cols + list_like_cols))\n",
        "\n",
        "print(\"List-like (excluidas):\", list_like_cols[:8])\n",
        "print(\"Excluidas por alta cardinalidad:\", high_card_cols[:8])\n",
        "print(\"Excluidas por texto largo:\", long_text_cols[:8])\n",
        "\n",
        "features = [c for c in _df.columns if c not in exclude_cols]\n",
        "\n",
        "X = _df[features].copy()\n",
        "y = _df[TARGET].copy()\n",
        "\n",
        "# Identificar columnas\n",
        "num_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "print(\"Numericas:\", len(num_features))\n",
        "print(\"Categoricas:\", len(cat_features))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Seleccion rapida de features numericas (SelectKBest)\n",
        "X_num = X[num_features].copy()\n",
        "X_num = X_num.fillna(X_num.median())\n",
        "\n",
        "k = min(20, X_num.shape[1])\n",
        "selector = SelectKBest(score_func=f_regression, k=k)\n",
        "selector.fit(X_num, y)\n",
        "\n",
        "scores = pd.Series(selector.scores_, index=X_num.columns).sort_values(ascending=False)\n",
        "print(\"Top features (numericas):\")\n",
        "print(scores.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Preprocesamiento (Pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Preprocesamiento: imputacion + escalado + one-hot\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, num_features),\n",
        "        ('cat', categorical_transformer, cat_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Preprocesador denso (para GaussianNB si se usa)\n",
        "categorical_transformer_dense = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
        "])\n",
        "\n",
        "preprocessor_dense = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, num_features),\n",
        "        ('cat', categorical_transformer_dense, cat_features)\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Baseline simple: predecir media\n",
        "baseline_pred = np.full_like(y_test, y_train.mean(), dtype=float)\n",
        "rmse = mean_squared_error(y_test, baseline_pred, squared=False)\n",
        "mae = mean_absolute_error(y_test, baseline_pred)\n",
        "r2 = r2_score(y_test, baseline_pred)\n",
        "print({\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Evaluacion de modelos de regresion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Funcion de evaluacion\n",
        "\n",
        "def eval_regression_models(models, X_train, X_test, y_train, y_test, preprocessor):\n",
        "    rows = []\n",
        "    for name, model in models:\n",
        "        pipe = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])\n",
        "        pipe.fit(X_train, y_train)\n",
        "        preds = pipe.predict(X_test)\n",
        "        rows.append({\n",
        "            'Model': name,\n",
        "            'RMSE': mean_squared_error(y_test, preds, squared=False),\n",
        "            'MAE': mean_absolute_error(y_test, preds),\n",
        "            'R2': r2_score(y_test, preds)\n",
        "        })\n",
        "    return pd.DataFrame(rows).sort_values(by='RMSE')\n",
        "\n",
        "voter = VotingRegressor(\n",
        "    estimators=[\n",
        "        ('lr', LinearRegression()),\n",
        "        ('rf', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)),\n",
        "        ('gbr', GradientBoostingRegressor(random_state=42))\n",
        "    ]\n",
        ")\n",
        "\n",
        "models_reg = [\n",
        "    ('LinearRegression', LinearRegression()),\n",
        "    ('Ridge', Ridge(alpha=1.0)),\n",
        "    ('Lasso', Lasso(alpha=0.001)),\n",
        "    ('ElasticNet', ElasticNet(alpha=0.001, l1_ratio=0.5)),\n",
        "    ('BayesianRidge', BayesianRidge()),\n",
        "    ('SGDRegressor', SGDRegressor(max_iter=2000, tol=1e-3, random_state=42)),\n",
        "    ('KernelRidge_RBF', KernelRidge(kernel='rbf', alpha=1.0, gamma=0.1)),\n",
        "    ('SVR_RBF', SVR(C=10, gamma='scale')),\n",
        "    ('KNN', KNeighborsRegressor(n_neighbors=10)),\n",
        "    ('DecisionTree', DecisionTreeRegressor(max_depth=12, random_state=42)),\n",
        "    ('RandomForest', RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)),\n",
        "    ('ExtraTrees', ExtraTreesRegressor(n_estimators=200, random_state=42, n_jobs=-1)),\n",
        "    ('GradientBoosting', GradientBoostingRegressor(random_state=42)),\n",
        "    ('AdaBoost', AdaBoostRegressor(random_state=42)),\n",
        "    ('MLPRegressor', MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)),\n",
        "    ('VotingRegressor', voter)\n",
        "]\n",
        "\n",
        "results_reg = eval_regression_models(models_reg, X_train, X_test, y_train, y_test, preprocessor)\n",
        "results_reg.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Ajuste de hiperparametros (subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Hiperparametros en modelos clave (grids pequenos por performance)\n",
        "\n",
        "param_grids = {\n",
        "    'Ridge': {\n",
        "        'model__alpha': [0.1, 1.0, 10.0]\n",
        "    },\n",
        "    'RandomForest': {\n",
        "        'model__n_estimators': [200, 400],\n",
        "        'model__max_depth': [10, 20, None],\n",
        "        'model__min_samples_split': [2, 5]\n",
        "    },\n",
        "    'SVR_RBF': {\n",
        "        'model__C': [1, 10],\n",
        "        'model__gamma': ['scale', 0.1]\n",
        "    },\n",
        "    'MLPRegressor': {\n",
        "        'model__hidden_layer_sizes': [(64, 32), (128, 64)],\n",
        "        'model__alpha': [0.0001, 0.001]\n",
        "    }\n",
        "}\n",
        "\n",
        "best_models = {}\n",
        "\n",
        "for name, model in models_reg:\n",
        "    if name not in param_grids:\n",
        "        continue\n",
        "    pipe = Pipeline(steps=[('preprocess', preprocessor), ('model', model)])\n",
        "    grid = GridSearchCV(pipe, param_grids[name], cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_models[name] = grid.best_estimator_\n",
        "    print(name, \"best RMSE:\", -grid.best_score_)\n",
        "\n",
        "best_models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Interpretabilidad (features y coeficientes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Interpretabilidad para el mejor modelo arbol y uno lineal\n",
        "\n",
        "def get_feature_names(preprocessor, num_features, cat_features):\n",
        "    # Numericas\n",
        "    feat_names = list(num_features)\n",
        "    # Categoricas one-hot\n",
        "    if cat_features:\n",
        "        ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
        "        cat_names = ohe.get_feature_names_out(cat_features).tolist()\n",
        "        feat_names.extend(cat_names)\n",
        "    return feat_names\n",
        "\n",
        "# Tomamos un modelo interpretable\n",
        "if 'RandomForest' in best_models:\n",
        "    model = best_models['RandomForest']\n",
        "else:\n",
        "    # fallback: entrena RF baseline\n",
        "    model = Pipeline(steps=[('preprocess', preprocessor), ('model', RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1))])\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "# Extraer importancias\n",
        "pre = model.named_steps['preprocess']\n",
        "rf = model.named_steps['model']\n",
        "\n",
        "feat_names = get_feature_names(pre, num_features, cat_features)\n",
        "importances = pd.Series(rf.feature_importances_, index=feat_names).sort_values(ascending=False)\n",
        "importances.head(15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Coeficientes de Ridge (lineal)\n",
        "pipe_ridge = Pipeline(steps=[('preprocess', preprocessor), ('model', Ridge(alpha=1.0))])\n",
        "pipe_ridge.fit(X_train, y_train)\n",
        "\n",
        "pre = pipe_ridge.named_steps['preprocess']\n",
        "model = pipe_ridge.named_steps['model']\n",
        "feat_names = get_feature_names(pre, num_features, cat_features)\n",
        "\n",
        "coef = pd.Series(model.coef_, index=feat_names).sort_values(key=np.abs, ascending=False)\n",
        "coef.head(15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Clasificacion auxiliar por segmentos de precio (aplica LDA/Naive Bayes/SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Crear segmentos de precio (bajo/medio/alto) para aplicar clasificacion\n",
        "_df_class = _df.copy()\n",
        "_df_class['price_segment'] = pd.qcut(_df_class['price_clean'], q=3, labels=['low', 'mid', 'high'])\n",
        "\n",
        "Xc = _df_class[features].copy()\n",
        "yc = _df_class['price_segment'].copy()\n",
        "\n",
        "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
        "    Xc, yc, test_size=0.2, random_state=42, stratify=yc\n",
        ")\n",
        "\n",
        "models_clf = [\n",
        "    ('LogisticRegression', LogisticRegression(max_iter=1000)),\n",
        "    ('KNN', KNeighborsClassifier(n_neighbors=7)),\n",
        "    ('SVM_RBF', SVC(C=5, gamma='scale', probability=True)),\n",
        "    ('NaiveBayes', GaussianNB()),\n",
        "    ('DecisionTree', DecisionTreeClassifier(max_depth=10, random_state=42)),\n",
        "    ('RandomForest', RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)),\n",
        "    ('GradientBoosting', GradientBoostingClassifier(random_state=42)),\n",
        "    ('AdaBoost', AdaBoostClassifier(random_state=42)),\n",
        "    ('MLP', MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=300, random_state=42))\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for name, model in models_clf:\n",
        "    # GaussianNB requiere datos densos\n",
        "    use_pre = preprocessor_dense if name == 'NaiveBayes' else preprocessor\n",
        "    pipe = Pipeline(steps=[('preprocess', use_pre), ('model', model)])\n",
        "    pipe.fit(Xc_train, yc_train)\n",
        "    preds = pipe.predict(Xc_test)\n",
        "    acc = accuracy_score(yc_test, preds)\n",
        "    f1 = f1_score(yc_test, preds, average='macro')\n",
        "    rows.append({'Model': name, 'Accuracy': acc, 'F1_macro': f1})\n",
        "\n",
        "results_clf = pd.DataFrame(rows).sort_values(by='F1_macro', ascending=False)\n",
        "results_clf.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# LDA para reduccion de dimensiones (visualizacion)\n",
        "# Nota: LDA requiere datos numericos -> usamos solo numericas imputadas\n",
        "\n",
        "num_only = _df_class[num_features].copy()\n",
        "num_only = num_only.fillna(num_only.median())\n",
        "\n",
        "lda = LinearDiscriminantAnalysis(n_components=2)\n",
        "X_lda = lda.fit_transform(num_only, _df_class['price_segment'])\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "for label in ['low', 'mid', 'high']:\n",
        "    mask = _df_class['price_segment'] == label\n",
        "    plt.scatter(X_lda[mask, 0], X_lda[mask, 1], s=10, alpha=0.5, label=label)\n",
        "plt.title('LDA - Price Segments')\n",
        "plt.xlabel('LD1')\n",
        "plt.ylabel('LD2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) Conclusiones y siguiente paso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "- Selecciona el mejor modelo de regresion segun RMSE/R2.\n",
        "- Reporta variables mas influyentes.\n",
        "- Prepara el caso de uso (usuario final) con el modelo ganador.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
