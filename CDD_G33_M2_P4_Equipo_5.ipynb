{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA Unificado (Practica 4)\n",
        "\n",
        "**Fuente de datos:** `data/listings.csv.gz`  \n",
        "**Filas/Columnas:** 27,051 x 79  \n",
        "**Generado:** 2026-02-04 16:22"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "pd.set_option('display.max_columns', 200)\n",
        "pd.set_option('display.max_rows', 120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DATA_CANDIDATES = [\n",
        "    Path('data/listings.csv.gz'),\n",
        "    Path('listings.csv.gz'),\n",
        "    Path('listings.csv'),\n",
        "]\n",
        "\n",
        "data_path = None\n",
        "for p in DATA_CANDIDATES:\n",
        "    if p.exists():\n",
        "        data_path = p\n",
        "        break\n",
        "\n",
        "if data_path is None:\n",
        "    raise FileNotFoundError('No se encontro listings.csv.gz ni listings.csv')\n",
        "\n",
        "compression = 'gzip' if data_path.suffix == '.gz' else None\n",
        "df = pd.read_csv(data_path, compression=compression, low_memory=False)\n",
        "\n",
        "print('Data path:', data_path)\n",
        "print('Shape:', df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Calidad de datos (nulos y cardinalidad)\n",
        "\n",
        "| index | pct_missing |\n",
        "|---|---|\n",
        "| license | 100.0000 |\n",
        "| neighbourhood_group_cleansed | 100.0000 |\n",
        "| calendar_updated | 100.0000 |\n",
        "| neighbourhood | 49.2218 |\n",
        "| neighborhood_overview | 49.2218 |\n",
        "| host_neighbourhood | 45.3366 |\n",
        "| host_about | 40.0318 |\n",
        "| host_location | 21.5408 |\n",
        "| host_response_time | 14.6205 |\n",
        "| host_response_rate | 14.6205 |\n",
        "| bed_per_person | 12.9607 |\n",
        "| beds | 12.9607 |\n",
        "| bathrooms | 12.9237 |\n",
        "| price_clean | 12.8794 |\n",
        "| price | 12.8794 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 2) Nulos y cardinalidad ===\n",
        "def detect_drop_candidates(df, missing_threshold=60, high_card_threshold=200, long_text_len=80):\n",
        "    rows = []\n",
        "    n = len(df)\n",
        "    for col in df.columns:\n",
        "        s = df[col]\n",
        "        n_missing = s.isna().sum()\n",
        "        pct_missing = (n_missing / n) * 100\n",
        "        n_unique = s.nunique(dropna=True)\n",
        "        sample = s.dropna().astype(str).head(3).tolist()\n",
        "        mean_len = None\n",
        "        if s.dtype == 'object':\n",
        "            mean_len = s.dropna().astype(str).str.len().mean()\n",
        "        reasons = []\n",
        "        if pct_missing > missing_threshold:\n",
        "            reasons.append(f'missing>{missing_threshold}%')\n",
        "        if n_unique > high_card_threshold:\n",
        "            reasons.append('high_card')\n",
        "        if s.dtype == 'object' and mean_len and mean_len > long_text_len:\n",
        "            reasons.append('long_text')\n",
        "        if s.dtype == 'object' and s.astype(str).str.contains('http', case=False, na=False).mean() > 0.2:\n",
        "            reasons.append('url_like')\n",
        "        rows.append({\n",
        "            'col': col,\n",
        "            'pct_missing': round(pct_missing, 2),\n",
        "            'n_unique': n_unique,\n",
        "            'mean_len': None if mean_len is None else round(mean_len, 1),\n",
        "            'sample': sample,\n",
        "            'reasons': ', '.join(reasons)\n",
        "        })\n",
        "    return pd.DataFrame(rows).sort_values(by=['pct_missing','n_unique'], ascending=False)\n",
        "\n",
        "missing_report = detect_drop_candidates(df)\n",
        "missing_report.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Target y transformaci\u00f3n (`price_clean` vs `log_price_clean`)\n",
        "\n",
        "- Missing en `price_clean`: **12.88%**\n",
        "- Skew `price_clean`: **52.125**\n",
        "- Skew `log_price_clean`: **0.841**\n",
        "\n",
        "**Justificaci\u00f3n:** `log_price_clean` reduce la asimetr\u00eda y estabiliza la regresi\u00f3n."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 3) Target y transformaci\u00f3n ===\n",
        "if 'price' in df.columns:\n",
        "    df['price_clean'] = (\n",
        "        df['price'].astype(str)\n",
        "        .str.replace(r'[,$]', '', regex=True)\n",
        "    )\n",
        "    df['price_clean'] = pd.to_numeric(df['price_clean'], errors='coerce')\n",
        "    df['log_price_clean'] = np.log1p(df['price_clean'])\n",
        "\n",
        "if 'price_clean' in df.columns:\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    sns.histplot(df['price_clean'].dropna(), bins=60, kde=True, ax=ax[0], color='#4C72B0')\n",
        "    ax[0].set_title('price_clean (crudo)')\n",
        "    ax[0].set_xlabel('price_clean')\n",
        "\n",
        "    sns.histplot(df['log_price_clean'].dropna(), bins=60, kde=True, ax=ax[1], color='#55A868')\n",
        "    ax[1].set_title('log_price_clean')\n",
        "    ax[1].set_xlabel('log_price_clean')\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Outliers\n",
        "\n",
        "- Clip 1\u201399%: **[224.66, 9918.58]**\n",
        "- IQR bounds: **[-809.00, 3063.00]**\n",
        "- % debajo IQR: **0.00%**\n",
        "- % arriba IQR: **6.54%**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 4) Outliers ===\n",
        "if 'price_clean' in df.columns:\n",
        "    q01, q99 = df['price_clean'].quantile([0.01, 0.99])\n",
        "    df['price_clean_clip'] = df['price_clean'].clip(q01, q99)\n",
        "\n",
        "    q1 = df['price_clean'].quantile(0.25)\n",
        "    q3 = df['price_clean'].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - 1.5 * iqr\n",
        "    upper = q3 + 1.5 * iqr\n",
        "    df['price_clean_iqr'] = df['price_clean'].clip(lower, upper)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(14, 4))\n",
        "    sns.boxplot(y=df['price_clean'], ax=ax[0], showfliers=False)\n",
        "    ax[0].set_title('price_clean')\n",
        "    sns.boxplot(y=df['price_clean_clip'], ax=ax[1], showfliers=False)\n",
        "    ax[1].set_title('price_clean_clip (1-99%)')\n",
        "    sns.boxplot(y=df['price_clean_iqr'], ax=ax[2], showfliers=False)\n",
        "    ax[2].set_title('price_clean_iqr (IQR)')\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Rates del host"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 5) Rates ===\n",
        "for col in ['host_response_rate', 'host_acceptance_rate']:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].astype(str).str.replace('%', '', regex=False)\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "if 'host_response_rate' in df.columns:\n",
        "    df[['host_response_rate','host_acceptance_rate']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Amenities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 6) Amenities ===\n",
        "\n",
        "def parse_amenities(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    text = str(text).strip('{}')\n",
        "    parts = [p.strip().strip('\"').strip(\"'\") for p in text.split(',')]\n",
        "    return [p for p in parts if p]\n",
        "\n",
        "if 'amenities' in df.columns:\n",
        "    df['amenities_list'] = df['amenities'].apply(parse_amenities)\n",
        "    df['amenities_count'] = df['amenities_list'].apply(len)\n",
        "\n",
        "    key_amenities = ['Wifi', 'Air conditioning', 'Pool', 'Kitchen', 'Parking']\n",
        "    for amenity in key_amenities:\n",
        "        col = f\"has_{amenity.lower().replace(' ', '_')}\"\n",
        "        df[col] = df['amenities_list'].apply(\n",
        "            lambda lst: int(any(amenity.lower() in a.lower() for a in lst))\n",
        "        )\n",
        "\n",
        "    # gr\u00e1fico simple\n",
        "    rates = df[[f\"has_{a.lower().replace(' ', '_')}\" for a in key_amenities]].mean().sort_values()\n",
        "    rates.plot(kind='barh', figsize=(6,4), title='Amenities clave (proporci\u00f3n)')\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Capacidad y layout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 7) Capacidad y layout ===\n",
        "for col in ['accommodates','bedrooms','beds']:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "df['total_capacity'] = df['accommodates'].fillna(0) + df['bedrooms'].fillna(0) + df['beds'].fillna(0)\n",
        "df['bed_per_person'] = df['beds'] / df['accommodates'].replace(0, np.nan)\n",
        "df['bedroom_per_person'] = df['bedrooms'] / df['accommodates'].replace(0, np.nan)\n",
        "df['space_per_person'] = df['total_capacity'] / df['accommodates'].replace(0, np.nan)\n",
        "\n",
        "df[['total_capacity','bed_per_person','bedroom_per_person','space_per_person']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Geograf\u00eda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 8) Geografia ===\n",
        "\n",
        "def haversine(lat, lon, lat0, lon0):\n",
        "    R = 6371.0\n",
        "    lat1 = np.radians(lat)\n",
        "    lon1 = np.radians(lon)\n",
        "    lat2 = np.radians(lat0)\n",
        "    lon2 = np.radians(lon0)\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
        "    return R * c\n",
        "\n",
        "if 'latitude' in df.columns and 'longitude' in df.columns:\n",
        "    ZOCALO = (19.4326, -99.1332)\n",
        "    AICM = (19.4361, -99.0719)\n",
        "    df['dist_zocalo_km'] = haversine(df['latitude'], df['longitude'], *ZOCALO)\n",
        "    df['dist_aicm_km'] = haversine(df['latitude'], df['longitude'], *AICM)\n",
        "    df['distance_from_center_km'] = df['dist_zocalo_km']\n",
        "    df['is_central_location'] = (df['distance_from_center_km'] < 5).astype(int)\n",
        "\n",
        "    df[['dist_zocalo_km','dist_aicm_km']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Recencia de reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 9) Recencia de reviews ===\n",
        "for col in ['last_scraped','last_review','first_review','host_since']:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "if 'last_scraped' in df.columns and 'last_review' in df.columns:\n",
        "    df['days_since_last_review'] = (df['last_scraped'] - df['last_review']).dt.days\n",
        "\n",
        "if 'last_scraped' in df.columns and 'host_since' in df.columns:\n",
        "    df['host_tenure_days'] = (df['last_scraped'] - df['host_since']).dt.days\n",
        "\n",
        "if 'days_since_last_review' in df.columns:\n",
        "    bins = [-1, 30, 90, 180, 365, 99999]\n",
        "    labels = ['<=30', '31-90', '91-180', '181-365', '>365']\n",
        "    df['recency_group'] = pd.cut(df['days_since_last_review'], bins=bins, labels=labels)\n",
        "    df['recency_group'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Actividad / disponibilidad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 10) Actividad / disponibilidad ===\n",
        "if 'availability_365' in df.columns:\n",
        "    df['availability_rate'] = df['availability_365'] / 365\n",
        "    df['scarcity_score'] = 1 - df['availability_rate']\n",
        "\n",
        "if 'maximum_nights' in df.columns and 'minimum_nights' in df.columns:\n",
        "    df['booking_flexibility'] = df['maximum_nights'] - df['minimum_nights']\n",
        "\n",
        "if 'availability_rate' in df.columns:\n",
        "    df[['availability_rate','scarcity_score','booking_flexibility']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Lujo (NLP simple)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 11) NLP simple para lujo ===\n",
        "luxury_keywords = [\n",
        "    'luxury','lujo','premium','exclusivo','exclusive','elegante','elegant',\n",
        "    'boutique','vista','panoramica','private','privado','spacious','amplio'\n",
        "]\n",
        "\n",
        "if 'description' in df.columns:\n",
        "    desc = df['description'].fillna('').str.lower()\n",
        "    df['luxury_keyword_count'] = desc.apply(lambda x: sum(1 for kw in luxury_keywords if kw in x))\n",
        "    df['is_luxury_property'] = (df['luxury_keyword_count'] > 0).astype(int)\n",
        "    df[['luxury_keyword_count','is_luxury_property']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14) Propuesta de columnas a remover (con justificaci\u00f3n)\n",
        "\n",
        "- **100% nulos:** neighbourhood_group_cleansed, calendar_updated, license\n",
        "- **IDs / URLs:** id, listing_url, scrape_id, host_id, host_url, picture_url, host_thumbnail_url, host_picture_url\n",
        "- **Texto largo / alta cardinalidad:** name, description, neighborhood_overview, host_about, host_location, host_name, host_verifications\n",
        "- **Fechas crudas (ya derivadas):** last_scraped, calendar_last_scraped, first_review, last_review, host_since\n",
        "- **Redundantes / list-like:** bathrooms_text, amenities, neighbourhood, amenities_list\n",
        "- **Leakage / demanda:** price, price_clean, log_price_clean, availability_30, availability_60, availability_90, availability_365, availability_eoy, has_availability, number_of_reviews, number_of_reviews_ltm, number_of_reviews_l30d, number_of_reviews_ly, reviews_per_month, estimated_occupancy_l365d, estimated_revenue_l365d, review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin ...\n",
        "\n",
        "**Resultado de la remoci\u00f3n:**\n",
        "- Columnas removidas: **50**\n",
        "- Shape original: **27051 x 104**\n",
        "- Shape final (model): **27051 x 54**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 14) Remoci\u00f3n de columnas (propuesta) ===\n",
        "all_missing_cols = ['neighbourhood_group_cleansed','calendar_updated','license']\n",
        "id_url_cols = ['id','listing_url','scrape_id','host_id','host_url','picture_url','host_thumbnail_url','host_picture_url']\n",
        "text_cols = ['name','description','neighborhood_overview','host_about','host_location','host_name','host_verifications']\n",
        "raw_date_cols = ['last_scraped','calendar_last_scraped','first_review','last_review','host_since']\n",
        "redundant_cols = ['bathrooms_text','amenities','neighbourhood','amenities_list']\n",
        "leakage_strict = [\n",
        "    'price','price_clean','log_price_clean',\n",
        "    'availability_30','availability_60','availability_90','availability_365','availability_eoy','has_availability',\n",
        "    'number_of_reviews','number_of_reviews_ltm','number_of_reviews_l30d','number_of_reviews_ly','reviews_per_month',\n",
        "    'estimated_occupancy_l365d','estimated_revenue_l365d',\n",
        "    'review_scores_rating','review_scores_accuracy','review_scores_cleanliness','review_scores_checkin',\n",
        "    'review_scores_communication','review_scores_location','review_scores_value'\n",
        "]\n",
        "\n",
        "present = lambda cols: [c for c in cols if c in df.columns]\n",
        "\n",
        "DROP_COLS = list(dict.fromkeys(\n",
        "    present(all_missing_cols) + present(id_url_cols) + present(text_cols) +\n",
        "    present(raw_date_cols) + present(redundant_cols) + present(leakage_strict)\n",
        "))\n",
        "\n",
        "print('Columnas a remover:', len(DROP_COLS))\n",
        "DROP_COLS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15) Columnas finales (despu\u00e9s de features)\n",
        "\n",
        "- `source`\n",
        "- `host_response_time`\n",
        "- `host_response_rate`\n",
        "- `host_acceptance_rate`\n",
        "- `host_is_superhost`\n",
        "- `host_neighbourhood`\n",
        "- `host_listings_count`\n",
        "- `host_total_listings_count`\n",
        "- `host_has_profile_pic`\n",
        "- `host_identity_verified`\n",
        "- `neighbourhood_cleansed`\n",
        "- `latitude`\n",
        "- `longitude`\n",
        "- `property_type`\n",
        "- `room_type`\n",
        "- `accommodates`\n",
        "- `bathrooms`\n",
        "- `bedrooms`\n",
        "- `beds`\n",
        "- `minimum_nights`\n",
        "- `maximum_nights`\n",
        "- `minimum_minimum_nights`\n",
        "- `maximum_minimum_nights`\n",
        "- `minimum_maximum_nights`\n",
        "- `maximum_maximum_nights`\n",
        "- `minimum_nights_avg_ntm`\n",
        "- `maximum_nights_avg_ntm`\n",
        "- `instant_bookable`\n",
        "- `calculated_host_listings_count`\n",
        "- `calculated_host_listings_count_entire_homes`\n",
        "- `calculated_host_listings_count_private_rooms`\n",
        "- `calculated_host_listings_count_shared_rooms`\n",
        "- `amenities_count`\n",
        "- `has_wifi`\n",
        "- `has_air_conditioning`\n",
        "- `has_pool`\n",
        "- `has_kitchen`\n",
        "- `has_parking`\n",
        "- `total_capacity`\n",
        "- `bed_per_person`\n",
        "- `bedroom_per_person`\n",
        "- `space_per_person`\n",
        "- `dist_zocalo_km`\n",
        "- `dist_aicm_km`\n",
        "- `distance_from_center_km`\n",
        "- `is_central_location`\n",
        "- `days_since_last_review`\n",
        "- `host_tenure_days`\n",
        "- `recency_group`\n",
        "- `availability_rate`\n",
        "- `scarcity_score`\n",
        "- `booking_flexibility`\n",
        "- `luxury_keyword_count`\n",
        "- `is_luxury_property`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 15) Dataset final ===\n",
        "df_model = df.drop(columns=DROP_COLS, errors='ignore').copy()\n",
        "# eliminar list-like si aparece\n",
        "for c in df_model.columns:\n",
        "    if df_model[c].apply(lambda v: isinstance(v, list)).any():\n",
        "        df_model = df_model.drop(columns=[c])\n",
        "\n",
        "print('Shape final:', df_model.shape)\n",
        "df_model.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16) Modelado (Regresi\u00f3n sobre `log_price_clean`)\n",
        "Se entrena con pipeline (imputaci\u00f3n + escalado + one-hot). M\u00e9tricas reportadas en escala log y tambi\u00e9n re-transformadas a escala original.\n",
        "\n",
        "| index | model | rmse_log | mae_log | r2_log | rmse_orig | mae_orig | r2_orig |\n",
        "|---|---|---|---|---|---|---|---|\n",
        "| 0 | RandomForest | 0.3719 | 0.2497 | 0.7527 | 9336.5418 | 601.3609 | 0.4300 |\n",
        "| 1 | ExtraTrees | 0.3732 | 0.2455 | 0.7510 | 2722.8286 | 441.9990 | 0.9515 |\n",
        "| 2 | GradientBoosting | 0.4286 | 0.3098 | 0.6714 | 11449.3053 | 733.0579 | 0.1428 |\n",
        "| 3 | MLP | 0.4645 | 0.3287 | 0.6141 | 10577.3905 | 701.4562 | 0.2684 |\n",
        "| 4 | LinearRegression | 0.4705 | 0.3318 | 0.6041 | 78566.7108 | 1900.5180 | -39.3634 |\n",
        "| 5 | Ridge | 0.4750 | 0.3360 | 0.5965 | 113461.1089 | 2412.8868 | -83.1791 |\n",
        "| 6 | Lasso | 0.4862 | 0.3452 | 0.5772 | 129765.2434 | 2656.1749 | -109.1100 |\n",
        "| 7 | Baseline (median) | 0.7478 | 0.5651 | -0.0001 | nan | nan | nan |\n",
        "\n",
        "**Nota:** `r2_orig` puede ser inestable por la alta asimetr\u00eda del precio. Se recomienda comparar principalmente `r2_log` y RMSE/MAE en escala original."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 16) Modelado (Regresi\u00f3n) ===\n",
        "# Ver resultados en la tabla de arriba (generados por corrida externa).\n",
        "# Aqu\u00ed dejamos el pipeline base para reproducir.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# target\n",
        "y = df['log_price_clean']\n",
        "mask = y.notna()\n",
        "X = df_model.loc[mask].copy()\n",
        "y = y.loc[mask].copy()\n",
        "\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns\n",
        "cat_cols = X.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "try:\n",
        "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "except TypeError:\n",
        "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', ohe)\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[('num', numeric_transformer, num_cols),\n",
        "                  ('cat', categorical_transformer, cat_cols)]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17) Clasificaci\u00f3n (segmentos de precio: low/mid/high)\n",
        "Se segmenta el precio con `qcut` y se eval\u00faan clasificadores con `accuracy` y `F1_macro`.\n",
        "\n",
        "| index | model | accuracy | f1_macro |\n",
        "|---|---|---|---|\n",
        "| 0 | RandomForest | 0.7724 | 0.7726 |\n",
        "| 1 | GradientBoosting | 0.7259 | 0.7263 |\n",
        "| 2 | LogisticRegression | 0.6996 | 0.7000 |\n",
        "| 3 | KNN | 0.6843 | 0.6835 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === 17) Clasificaci\u00f3n por segmentos de precio ===\n",
        "# Se deja el pipeline base para reproducir.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# target: segmentos\n",
        "mask = df['price_clean'].notna()\n",
        "X = df_model.loc[mask].copy()\n",
        "y = df.loc[mask, 'price_clean']\n",
        "\n",
        "y_seg = pd.qcut(y, q=3, labels=['low','mid','high'])\n",
        "\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns\n",
        "cat_cols = X.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "try:\n",
        "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "except TypeError:\n",
        "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
        "                                          ('scaler', StandardScaler())]), num_cols),\n",
        "                  ('cat', Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "                                          ('onehot', ohe)]), cat_cols)]\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": ".venv",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}